# -*- coding: utf-8 -*-
"""Churn_prediction.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1PW6JZsit3KcSDF6OXC__4szXZ-rogIDD
"""

#Mounting drive to upload dataset
from google.colab import drive
drive.mount('/content/drive')

"""## Import Libraries"""

import numpy as np    # linear algebra
import pandas as pd   # data processing, CSV file I/O (e.g. pd.read_csv)
import seaborn as sns # For creating plots
import matplotlib.ticker as mtick # For specifying the axes tick format
import matplotlib.pyplot as plt
from sklearn.ensemble import RandomForestClassifier
from sklearn.ensemble import GradientBoostingClassifier
from sklearn.model_selection import train_test_split
from sklearn.metrics import accuracy_score
sns.set(style = 'white')

# Input data files are available in the "../churn_prediction" directory.

import os
print(os.listdir("/content/drive/MyDrive/Churn_Prediction_Project"))

# Any results we write to the current directory are saved as output.

# loading dataset using pandas
df = pd.read_csv('/content/drive/MyDrive/Churn_Prediction_Project/WA_Fn-UseC_-Telco-Customer-Churn.csv')
df.head()

# printing Shape of dataframe
df.shape

"""## Metadata info"""

df.info()

# Converting Total Charges to a numerical data type.
df.TotalCharges = pd.to_numeric(df.TotalCharges, errors='coerce')

# Passed a dictionary to astype() function
df = df.astype({"customerID":'category',
                "gender":'category',
                "SeniorCitizen":'category',
                "Partner":'category',
                "Dependents":'category',
                "tenure":'float64',
                "PhoneService":'category',
                "MultipleLines":'category',
                "InternetService":'category',
                "OnlineSecurity":'category',
                "OnlineBackup":'category',
                "DeviceProtection":'category',
                "TechSupport":'category',
                "StreamingTV":'category',
                "StreamingMovies":'category',
                "Contract":'category',
                "PaperlessBilling":'category',
                "PaymentMethod":'category',
                "MonthlyCharges": 'float64',
                #"TotalCharges": 'float64'
               })

df.nunique()

"""<h4>Target Variable:<br><br>
    Churn: Indicates if the customer has churned (Yes or No)
</h4>
<h4>Numerical Attributes:<br><br>
    1. MonthlyCharges: The monthly fee billed to the customer<br><br>
    2. TotalCharges: The cumulative amount billed to the customer<br>
</h4>
<h4>Categorical Attributes:
    1. customerID: A unique identifier for each customer<br><br>
    2. gender: The customer's gender (male or female)<br><br>
    3. SeniorCitizen: Indicates if the customer is a senior citizen (1, 0)<br><br>
    4. Partner: Indicates if the customer has a partner (Yes, No)<br><br>
    5. Dependents: Indicates if the customer has dependents (Yes, No)<br><br>
    6. Tenure: The duration in months that the customer has been with the company<br><br>
    7. PhoneService: Indicates if the customer has phone service (Yes, No)<br><br>
    8. MultipleLines: Indicates if the customer has multiple telephone lines (Yes, No, No phone service)<br><br>
    9. InternetService: The type of internet service the customer subscribes to (DSL, Fiber optic, No)<br><br>
    10. OnlineSecurity: Indicates if the customer has online security services (Yes, No, No internet service)<br><br>
    11. OnlineBackup: Indicates if the customer uses online backup services (Yes, No, No internet service)<br><br>
    12. DeviceProtection: Indicates if the customer's devices are insured (Yes, No, No internet service)<br><br>
    13. TechSupport: Indicates if the customer has technical support (Yes, No, No internet service)<br><br>
    14. StreamingTV: Indicates if the customer subscribes to streaming TV services (Yes, No, No internet service)<br><br>
    15. StreamingMovies: Indicates if the customer subscribes to streaming movies services (Yes, No, No internet service)<br><br>
    16. Contract: The type of customer contract (Month-to-month, One year, Two year)<br><br>
    17. PaperlessBilling: Indicates if the customer uses paperless billing (Yes, No)<br><br>
    18. PaymentMethod: The methods by which the customer makes payments (Electronic check, Mailed check, Bank transfer (automatic), Credit card (automatic))
</h4>

## Handling  Null values
"""

# Checking for null values
df.isnull().sum()

# Percentage of missing values
df.isnull().sum() * 100 / len(df)

"""After reviewing the provided output, it is evident that there are 11 missing values, constituting only 0.15% of the total values for the "Total Charges" variable in our dataset. Therefore, we can employ simple mean imputation to fill these missing values."""

# imputing missing values with mean of TotalCharges column
df.TotalCharges.fillna(df.TotalCharges.mean(), inplace=True)

"""## Exploratory Data Analysis of Telecom churn data

### Checking for class imbalance in data

<h4>Our objective is to determine whether a client departed from the company in the prior month, which presents a binary classification challenge due to a somewhat uneven distribution of the target variable:
<br><br>
<li> Churn: No - 72.4% </li><br>
<li> Churn: Yes - 27.6% </li>
</h4>
"""

#  Analysing Class Distribution
df.Churn.value_counts()

# Define the bar_plot function
def bar_plot(df, column):
    # Create a bar plot using seaborn's countplot function
    ax = sns.countplot(y=column, data=df)

    # Set plot title and x-axis label
    plt.title('Distribution of Configurations')
    plt.xlabel('Number of Axles')

    # Calculate the total number of data points
    total = len(df[column])

    # Add percentage labels to each bar
    for p in ax.patches:
        percentage = '{:.1f}%'.format(100 * p.get_width()/total)
        x = p.get_x() + p.get_width() + 0.02
        y = p.get_y() + p.get_height()/2
        ax.annotate(percentage, (x, y), size=6, color='black')

    # Add percentage labels to each box
    for i, box in enumerate(ax.artists):
        height = box.get_height()
        ax.text(i-0.1, height*0.5, '{:.1f}%'.format(height/total*100), ha='center', size=10, color='black')
    # Display the plot
    plt.show()

# Call the bar_plot function on the DataFrame and Churn Column
bar_plot(df, "Churn")

"""# **Numeric Attributes**
The dataset comprises solely three numerical columns: tenure, monthly charges, and total charges.

Based on the depicted plots, we can derive the following conclusions:

*   Recent users exhibit a higher likelihood of churning.
*   Users with elevated MonthlyCharges demonstrate a heightened tendency to churn.
* TotalCharges display a similar behavior for both categories.

####  Plotting Histogram
"""

# Plot a density plot and histogram of  arrival delays
sns.histplot(df['TotalCharges'], bins=int(180/5), color='darkblue',
             kde=False, edgecolor='black')

sns.kdeplot(df['TotalCharges'], color='darkblue', linewidth=4, ax=plt.gca())

# Increase outer border size of plot
plt.rcParams['axes.linewidth'] = 2.0
plt.gca().spines['bottom'].set_color('black')
plt.gca().spines['left'].set_color('black')
plt.gca().spines['right'].set_color('black')
plt.gca().spines['top'].set_color('black')

# Display the plot
plt.show()

# Plot a density plot and histogram of  arrival delays
sns.histplot(df['MonthlyCharges'], bins=int(180/5), color='darkblue',
             kde=False, edgecolor='black')

sns.kdeplot(df['MonthlyCharges'], color='darkblue', linewidth=4, ax=plt.gca())

# Increase outer border size of plot
plt.rcParams['axes.linewidth'] = 2.0
plt.gca().spines['bottom'].set_color('black')
plt.gca().spines['left'].set_color('black')
plt.gca().spines['right'].set_color('black')
plt.gca().spines['top'].set_color('black')

# Display the plot
plt.show()

# Plot a density plot and histogram of  arrival delays
sns.histplot(df['tenure'], bins=int(180/5), color='darkblue',
             kde=False, edgecolor='black')

sns.kdeplot(df['tenure'], color='darkblue', linewidth=4, ax=plt.gca())

# Increase outer border size of plot
plt.rcParams['axes.linewidth'] = 2.0
plt.gca().spines['bottom'].set_color('black')
plt.gca().spines['left'].set_color('black')
plt.gca().spines['right'].set_color('black')
plt.gca().spines['top'].set_color('black')

# Display the plot
plt.show()

"""####  Generating Pairplot"""

# Create a PairGrid object with the MonthlyCharges and TotalCharges
g = sns.PairGrid(df, y_vars=["tenure"], x_vars=["MonthlyCharges", "TotalCharges"], height=4.4, hue="Churn", aspect=1.1)

# Add scatterplots to the PairGrid object
ax = g.map(plt.scatter, alpha=0.6)

# Increase outer border size of plot
plt.rcParams['axes.linewidth'] = 2.0
plt.gca().spines['bottom'].set_color('black')
plt.gca().spines['left'].set_color('black')
plt.gca().spines['right'].set_color('black')
plt.gca().spines['top'].set_color('black')

# Display the plot
plt.show()

# Create a PairGrid object with the MonthlyCharges vs TotalCharges and tenure variables
g = sns.PairGrid(df, y_vars=["MonthlyCharges"], x_vars=["tenure", "TotalCharges"], height=4.5, hue="Churn", aspect=1.2)
ax = g.map(plt.scatter, alpha=0.7)

# Create a PairGrid object with the TotalCharges vs MonthlyCharges and tenure variables
g = sns.PairGrid(df, y_vars=["TotalCharges"], x_vars=["tenure", "MonthlyCharges"], height=4.4, hue="Churn", aspect=1.2)
ax = g.map(plt.scatter, alpha=0.7)

# Calculate new features
df['total_charges_to_tenure_ratio'] = df['TotalCharges'] / df['tenure'] # Calculate the ratio of total charges to tenure
df['monthly_charges_diff'] = df['MonthlyCharges'] - df['total_charges_to_tenure_ratio'] # Calculate the difference between monthly charges and the ratio

# Create a KDE plot of the 'monthly_charges_diff' column
sns.kdeplot(data=df, x='monthly_charges_diff', fill=True, color='darkblue', legend=False)

# Save the figure with a custom filename
plt.savefig('kde_plot_monthly_charges_diff.png')

"""
### **Categorical Attributes in Telecom Data**
This dataset comprises 16 categorical features:




*  Six binary features with options of Yes or No.
*  Nine features with three distinct values each, representing different categories.

* One feature with four unique values, providing additional categorical information.







"""

# Create a figure with 2 rows and 3 columns
fig, axes = plt.subplots(2, 3, figsize=(12, 7), sharey=True, gridspec_kw={'wspace':0.3, 'hspace':0.5} )

# Create count plots for each variable
sns.countplot(data=df, x="gender", ax=axes[0,0])
sns.countplot(data=df, x="SeniorCitizen", ax=axes[0,1])
sns.countplot(data=df, x="Partner", ax=axes[0,2])
sns.countplot(data=df, x="Dependents", ax=axes[1,0])
sns.countplot(data=df, x="PhoneService", ax=axes[1,1])
sns.countplot(data=df, x="PaperlessBilling", ax=axes[1,2])
# Save the figure with a count_plots filename
fig.savefig("count_plots.png")

# Call the bar_plot function on the DataFrame and  each categorical column
bar_plot(df, "gender")
bar_plot(df, "SeniorCitizen")
bar_plot(df, "Partner")
bar_plot(df, "Dependents")
bar_plot(df, "PhoneService")
bar_plot(df, "PaperlessBilling")

"""### Partner and Dependents Plot"""

sns.countplot(x="Partner", data=df, hue = 'Dependents')

sns.countplot(x="SeniorCitizen", data=df, hue = 'gender')

"""### Senior Citizen and Dependent"""

sns.countplot(x="SeniorCitizen", data=df, hue = 'Dependents')

"""### Phone and Internet services Analysis"""

bar_plot(df, "MultipleLines")

# Countplot of churn among  customers with Multiplelines of phone
sns.countplot(x="MultipleLines", data=df, hue = 'Churn')

"""

*   A small number of customers lack phone service.

*   Customers who subscribe to multiple lines tend to exhibit a marginally higher churn rate.

"""

# Plot customers with different InternetService
bar_plot(df, "InternetService")

# Counting churn rate among customers with InternetService
sns.countplot(x="InternetService", data=df, hue = 'Churn')

"""

*  
Customers who do not have internet service experience a very minimal churn rate.

* Customers who have a fiber connection are more likely to churn compared to those with a DSL connection.  

"""

# Define the list of columns to be melted
cols = ["OnlineSecurity", "OnlineBackup", "DeviceProtection", "TechSupport", "StreamingTV", "StreamingMovies"]

# Filter out the rows with "No" InternetService and melt the dataframe
df1 = pd.melt(df[df["InternetService"] != "No"][cols]).rename({'value': 'Has service'}, axis=1)

# Create a figure with a specified size
plt.figure(figsize=(10, 4.5))

# Plot the count of customers for each additional service
ax = sns.countplot(data=df1, x='variable', hue='Has service')

# Set the x and y labels of the plot
ax.set(xlabel='Additional service', ylabel='Num of customers')

# Display the plot
plt.show()

"""###  Customer distribution based on Payment Method"""

# Bar plot of paymentMethod
bar_plot(df, "PaymentMethod")

# Countplot of churn rate based on payment method
ax = sns.countplot(x="PaymentMethod", data=df, hue = 'Churn')

# Set the x-axis tick labels to rotate and display vertically
plt.xticks(rotation=45, ha='right')

# Set the title and labels of the plot
plt.title("Churn rate based on payment method")
plt.xlabel("Payment Method")
plt.ylabel("Num of customers")

# Display the plot
plt.show()

# Printing all columns dataframe
df.columns

"""## Correlation between features of dataset"""

plt.figure(figsize=(12, 6))
df_corr = df.apply(lambda x: pd.factorize(x)[0])
ax = sns.heatmap(df_corr.corr(), xticklabels=df_corr.columns, yticklabels=df_corr.columns,
                 linewidths=.2, cmap="YlGnBu")

plt.figure(figsize=(15, 10))
sns.heatmap(df_corr.corr(), annot=True)

"""## Feature Importance Analysis"""

params = {'random_state': 0, 'n_jobs': 4, 'n_estimators': 5000, 'max_depth': 8}
# One-hot encoding
df = pd.get_dummies(df)
# Dropping redundant columns (for features with two unique values)
drop = ['Churn_Yes', 'Churn_No', 'gender_Female', 'Partner_No',
        'Dependents_No', 'PhoneService_No', 'PaperlessBilling_No']
x, y = df.drop(drop,axis=1), df['Churn_Yes']
# Fit RandomForest Classifier
clf = RandomForestClassifier(**params)
clf = clf.fit(x, y)
# Plotting  features importance
imp = pd.Series(data=clf.feature_importances_, index=x.columns).sort_values(ascending=False)
plt.figure(figsize=(10,12))
plt.title("Feature importance")
ax = sns.barplot(y=imp.index, x=imp.values, palette="Blues_d", orient='h')

"""### **Oversampling Technique**
 The Synthetic Minority Oversampling Technique (SMOTE) is a widely used method to address imbalanced datasets by creating synthetic data points for the minority class, thereby oversampling that particular class.
"""

from imblearn.over_sampling import SMOTE
sm = SMOTE(random_state=0)
X_resampled, y_resampled = sm.fit_resample(x, y)

y_resampled.value_counts()

"""## Splitting Data into Train and Test
Divides data into Training and Testing Subsets
"""

X_train, X_test, y_train, y_test = train_test_split(X_resampled, y_resampled, test_size = 0.15, random_state=42)

# Import necessary libraries
from sklearn.linear_model import LogisticRegression
from sklearn.metrics import confusion_matrix, precision_score, recall_score, f1_score, accuracy_score

# Create and train the Logistic Regression model
lr_model = LogisticRegression(max_iter=3000)
lr_model.fit(X_train, y_train)

# Make predictions
lr_predictions = lr_model.predict(X_test)

# Print evaluation metrics
print("Logistic Regression Evaluation Metrics:")
print("Confusion Matrix : \n", confusion_matrix(y_test, lr_predictions))
print("Accuracy: ", accuracy_score(y_test, lr_predictions))
print("Precision: ", precision_score(y_test, lr_predictions, average=None))
print("Recall: ", recall_score(y_test, lr_predictions, average=None))
print("F1 Score: ", f1_score(y_test, lr_predictions, average=None))

# Create and train the Random Forest model
rf_model = RandomForestClassifier(random_state=0, n_estimators=5000, max_depth=8)
rf_model.fit(X_train, y_train)

# Make predictions
rf_predictions = rf_model.predict(X_test)

# Print evaluation metrics
print("\nRandom Forest Evaluation Metrics:")
print("Confusion Matrix : \n", confusion_matrix(y_test, rf_predictions))
print("Accuracy: ", accuracy_score(y_test, rf_predictions))
print("Precision: ", precision_score(y_test, rf_predictions, average=None))
print("Recall: ", recall_score(y_test, rf_predictions, average=None))
print("F1 Score: ", f1_score(y_test, rf_predictions, average=None))

# Create and train the Gradient Boosting model
gb_model = GradientBoostingClassifier(random_state=42,learning_rate=0.1, n_estimators=6000, max_depth=10)
gb_model.fit(X_train, y_train)

# Make predictions
gb_predictions = gb_model.predict(X_test)

# Print evaluation metrics
print("\nGradient Boosting Evaluation Metrics:")
print("Confusion Matrix : \n", confusion_matrix(y_test, gb_predictions))
print("Accuracy: ", accuracy_score(y_test, gb_predictions))
print("Precision: ", precision_score(y_test, gb_predictions, average=None))
print("Recall: ", recall_score(y_test, gb_predictions, average=None))
print("F1 Score: ", f1_score(y_test, gb_predictions, average=None))

# Compare different models
models = [lr_model, rf_model, gb_model]
model_names = ['Logistic Regression', 'Random Forest', 'Gradient Boosting']

for i, model in enumerate(models):
    y_pred = model.predict(X_test)
    print(f"\n{model_names[i]} Evaluation Metrics:")
    print(f"Accuracy: {accuracy_score(y_test, y_pred)}")
    print(f"Precision: {precision_score(y_test, y_pred, average=None)}")
    print(f"Recall: {recall_score(y_test, y_pred, average=None)}")
    print(f"F1 Score: {f1_score(y_test, y_pred, average=None)}")

"""### **Models**
To demonstrate the basic model and its predictions, a GradientBoostingClassifier model has been implemented for starters.
"""

# Installing required modules
!pip install scikit-learn

# Importing required modules
from sklearn.linear_model import LogisticRegression
from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier
from sklearn.metrics import classification_report

# Models used for classification
models = {
    "Logistic Regression": LogisticRegression(random_state=42),
    "Random Forest": RandomForestClassifier(random_state=42),
    "Gradient Boosting": GradientBoostingClassifier(random_state=42)
}

# Training and evaluating each model
for name, model in models.items():
    print(f"Training {name}...")
    model.fit(X_train, y_train)
    y_pred = model.predict(X_test)

# Evaluation report of model
    accuracy = accuracy_score(y_test, y_pred)
    recall = recall_score(y_test, y_pred, average=None)
    precision = precision_score(y_test, y_pred, average=None)
    f1 = f1_score(y_test, y_pred, average=None)
    conf_matrix = confusion_matrix(y_test, y_pred)
    class_report = classification_report(y_test, y_pred)

print(f"\n{name} Results:")
    print(f"Accuracy: {accuracy}")
    print(f"Recall: {recall}")
    print(f"Precision: {precision}")
    print(f"F1 Score: {f1}")
    print(f"Confusion Matrix:\n{conf_matrix}")
    print(f"Classification Report:\n{class_report}")
    print("=" * 50)

clf_forest = GradientBoostingClassifier()
clf_forest.fit(X_train, y_train)

"""Model performance on training subset"""

pred = clf_forest.predict(X_train)
accuracy_score(y_train, pred)

"""Model performance on testing subset"""

pred_test = clf_forest.predict(X_test)
accuracy_score(y_test, pred_test)

"""## Confusion Martrix for Churn prediction"""

from sklearn.metrics import confusion_matrix
from sklearn.metrics import classification_report

# actual values of customer churn
actual = y_test
#predicted values of customer churn
predicted = pred_test

# printing confusion matrix
matrix = confusion_matrix(actual,predicted, labels=[1,0])
print('Confusion matrix : \n',matrix)

# printing outcome values
tp, fn, fp, tn = confusion_matrix(actual,predicted,labels=[1,0]).reshape(-1)
print('Outcome values : \n', tp, fn, fp, tn)

# classification report for Precision, Recall f1-score and Accuracy
matrix = classification_report(actual,predicted,labels=[1,0])
print('Classification report : \n',matrix)

"""## Evaluation"""

y_true = y_test
y_pred = pred_test
# Confusion Matrix
from sklearn.metrics import confusion_matrix
confusion_matrix(y_true, y_pred)

# Accuracy on test data
from sklearn.metrics import accuracy_score
accuracy_score(y_true, y_pred)

# Recall
from sklearn.metrics import recall_score
recall_score(y_true, y_pred, average=None)

# Precision
from sklearn.metrics import precision_score
precision_score(y_true, y_pred, average=None)

# F1 Score
from sklearn.metrics import f1_score
f1_score(y_true, y_pred, average=None)